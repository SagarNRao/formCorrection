{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pose Form Checker – **RPi 4B + MediaMTX (No Errors)**\n",
        "\n",
        "*Laptop: MediaMTX + FFmpeg push → RPi: MediaPipe + live overlay.*\n",
        "\n",
        "---\n",
        "\n",
        "## Start Laptop\n",
        "1. Run `start_mediamtx.bat` → leave open\n",
        "2. Run `push_webcam.bat` → leave open\n",
        "\n",
        "## RPi\n",
        "Run cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "from IPython import display as ipydisplay\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "RTSP_URL = \"rtsp://10.227.207.170:8554/webcam\"\n",
        "\n",
        "# Suppress all H.264 warnings\n",
        "os.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = (\n",
        "    \"rtsp_transport;tcp|\"\n",
        "    \"fflags;nobuffer+discardcorrupt|\"\n",
        "    \"flags;low_delay|\"\n",
        "    \"max_delay;0|\"\n",
        "    \"probesize;32|\"\n",
        "    \"analyzeduration;0|\"\n",
        "    \"err_detect;ignore_err\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Reference (Run Once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1762501091.738311    2776 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1762501091.846852    2776 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "squat.pkl → 51 frames\n",
            "push-up.pkl → 74 frames\n",
            "romanian_deadlift.pkl → 27 frames\n",
            "plank.pkl → 39 frames\n",
            "deadlift.pkl → 34 frames\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def extract_all_references_from_folder(folder='exercises', target_fps=10.0):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Created {folder}/ – add your .mp4 files\")\n",
        "        return\n",
        "\n",
        "    video_files = [f for f in glob.glob(os.path.join(folder, \"*.*\")) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "    if not video_files:\n",
        "        print(\"No videos in exercises/\")\n",
        "        return\n",
        "\n",
        "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
        "    for video_path in video_files:\n",
        "        name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "        interval = max(1, int(fps / target_fps))\n",
        "        refs, cnt = [], 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            if cnt % interval == 0:\n",
        "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                res = pose.process(rgb)\n",
        "                if res.pose_landmarks:\n",
        "                    refs.append([(l.x, l.y, l.z, l.visibility) for l in res.pose_landmarks.landmark])\n",
        "            cnt += 1\n",
        "        cap.release()\n",
        "        with open(f\"{name}.pkl\", 'wb') as f:\n",
        "            pickle.dump(refs, f)\n",
        "        print(f\"{name}.pkl → {len(refs)} frames\")\n",
        "    pose.close()\n",
        "    print(\"Done!\")\n",
        "\n",
        "extract_all_references_from_folder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Live Form Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_exercise(exercise_name: str):\n",
        "    pkl_path = f\"{exercise_name}.pkl\"\n",
        "    if not os.path.exists(pkl_path):\n",
        "        print(f\"No reference: {pkl_path}\")\n",
        "        return\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        ref_seq = pickle.load(f)\n",
        "\n",
        "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
        "\n",
        "    # GStreamer: hardware decode, no warnings\n",
        "    gst = (\n",
        "        f\"rtspsrc location={RTSP_URL} latency=100 protocols=tcp ! \"\n",
        "        \"rtph264depay ! h264parse ! v4l2h264dec ! \"\n",
        "        \"videoconvert ! video/x-raw,format=BGR,width=640,height=480,framerate=15/1 ! \"\n",
        "        \"appsink drop=true max-buffers=1 sync=false\"\n",
        "    )\n",
        "    cap = cv2.VideoCapture(gst, cv2.CAP_GSTREAMER)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"GStreamer failed → FFmpeg\")\n",
        "        cap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)\n",
        "        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Stream failed\")\n",
        "        return\n",
        "\n",
        "    print(\"Connected! Starting overlay...\")\n",
        "    display_handle = ipydisplay.display(ipydisplay.Image(data=b''), display_id=True)\n",
        "\n",
        "    ref_idx = 0\n",
        "    last_time = time.time()\n",
        "    frame_delay = 1.0 / 10\n",
        "    angle_thr = 20\n",
        "    frame_cnt = 0\n",
        "\n",
        "    def angle(a, b, c):\n",
        "        a, b, c = np.array(a), np.array(b), np.array(c)\n",
        "        ab, bc = a-b, c-b\n",
        "        cos = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc) + 1e-6)\n",
        "        return np.degrees(np.arccos(np.clip(cos, -1, 1)))\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: continue\n",
        "            if frame_cnt % 3 != 0:\n",
        "                frame_cnt += 1\n",
        "                continue\n",
        "            frame_cnt += 1\n",
        "\n",
        "            h, w, _ = frame.shape\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = pose.process(rgb)\n",
        "\n",
        "            canvas = np.zeros((h, w, 3), np.uint8)\n",
        "\n",
        "            # Blue: Reference\n",
        "            ref = ref_seq[ref_idx]\n",
        "            for a, b in mp_pose.POSE_CONNECTIONS:\n",
        "                if ref[a][3] > 0.1 and ref[b][3] > 0.1:\n",
        "                    cv2.line(canvas, (int(ref[a][0]*w), int(ref[a][1]*h)), (int(ref[b][0]*w), int(ref[b][1]*h)), (255, 0, 0), 2)\n",
        "\n",
        "            # Green/Red: User\n",
        "            if results.pose_landmarks:\n",
        "                user = [(l.x, l.y, l.z, l.visibility) for l in results.pose_landmarks.landmark]\n",
        "                for a, b in mp_pose.POSE_CONNECTIONS:\n",
        "                    if all(x[3] > 0.1 for x in [ref[a], ref[b], user[a], user[b]]):\n",
        "                        parent = next((c[0] for c in mp_pose.POSE_CONNECTIONS if c[1] == b), None)\n",
        "                        if parent and ref[parent][3] > 0.1 and user[parent][3] > 0.1:\n",
        "                            ra = angle((ref[a][0], ref[a][1]), (ref[b][0], ref[b][1]), (ref[parent][0], ref[parent][1]))\n",
        "                            ua = angle((user[a][0], user[a][1]), (user[b][0], user[b][1]), (user[parent][0], user[parent][1]))\n",
        "                            color = (0, 255, 0) if abs(ra - ua) < angle_thr else (0, 0, 255)\n",
        "                            cv2.line(canvas, (int(user[a][0]*w), int(user[a][1]*h)), (int(user[b][0]*w), int(user[b][1]*h)), color, 2)\n",
        "\n",
        "            _, buf = cv2.imencode('.jpeg', canvas, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
        "            display_handle.update(ipydisplay.Image(data=buf.tobytes()))\n",
        "\n",
        "            if time.time() - last_time >= frame_delay:\n",
        "                last_time = time.time()\n",
        "                ref_idx = (ref_idx + 1) % len(ref_seq)\n",
        "\n",
        "            time.sleep(0.01)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nStopped\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "        pose.close()\n",
        "        ipydisplay.clear_output()\n",
        "        print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Workout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "perform_exercise('romanian_deadlift')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
